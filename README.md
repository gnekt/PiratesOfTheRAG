# Pirates of the RAG
**Adaptively Attacking LLMs to Leak Knowledge Bases**
> *Accepted at European Conference on Artificial Intelligence (ECAI) 2025*
> 
**Authors**  
<p align='center' style="text-align:center;font-size:1em;">
    <a href="https://collectionless.ai/post/christian_dimaio/">Christian Di Maio</a>&nbsp;,&nbsp;
    <a>Cristian Cosci</a>&nbsp;,&nbsp;
    <a href="https://scholar.google.com/citations?user=kZFskCoAAAAJ&hl=en">Marco Maggini</a>&nbsp;&nbsp;
    <a href="https://scholar.google.com/citations?user=9a1nVKwAAAAJ&hl=en">Valentina Poggioni</a>&nbsp;&nbsp;
    <a href="https://collectionless.ai/post/a-stefano_melacci/">Stefano Melacci</a>&nbsp;&nbsp;
    <br/> 
  University of Pisa (Italy), University of Siena (Italy), University of Perugia (Italy)
  <br/> 
</p>



[![ECAI 2025](https://img.shields.io/badge/ECAI-2025-blue)](#)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

This repository provides the official implementation of our paper: **"Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases"**. We present a novel, blackâ€‘box, adaptive, and fully automated attack against Retrievalâ€‘Augmented Generation (RAG) systems, demonstrating how to extract private knowledge-base contents via relevanceâ€‘driven query crafting.

<p align="center">
  <img src="/imgs/6e4eca81.png" alt="Attack Overview" width="80%">
</p>

## ğŸ“– Table of Contents

- [Pirates of the RAG](#pirates-of-the-rag)
  - [ğŸ“– Table of Contents](#-table-of-contents)
  - [ğŸ”‘ Key Features](#-key-features)
  - [Algorithm: Pirates of the RAG](#algorithm-pirates-of-the-rag)
  - [ğŸ› ï¸ Prerequisites](#ï¸-prerequisites)
  - [ğŸ’¾ Installation](#-installation)
  - [ğŸ—„ï¸ Preparing Knowledge Bases](#ï¸-preparing-knowledge-bases)
  - [ğŸš€ Running Experiments](#-running-experiments)
    - [1. Launch Target RAG Agents](#1-launch-target-rag-agents)
    - [2. Launch Attacker's Oracle LLM](#2-launch-attackers-oracle-llm)
    - [3. Run the Attack](#3-run-the-attack)
    - [4. Analyze Results](#4-analyze-results)
  - [ğŸ›¡ï¸ Testing Llama Guard Defense](#ï¸-testing-llama-guard-defense)
  - [ğŸ“‚ Repository Structure](#-repository-structure)
  - [ğŸ“‘ Citation](#-citation)
  - [âš ï¸ Ethical Considerations](#ï¸-ethical-considerations)
  - [ğŸ“œ License](#-license)

---

## ğŸ”‘ Key Features

* **Fully Automated & Adaptive**: Runs without human supervision, adapting queries dynamically based on leaked data.
* **Blackâ€‘Box Approach**: Requires no internal knowledge of the target RAGâ€™s architecture or prompts.
* **Openâ€‘Source & Locally Runnable**: Powered by consumerâ€‘grade open models (e.g., Llama 3.2 1B) and embedder, no proprietary APIs.
* **Relevanceâ€‘Based Anchor Mechanism**: Uses dynamically updated topic anchors to guide exploration of hidden KB chunks.
* **Proven Effectiveness**: Outperforms existing extraction methods across multiple RAG configurations.

<details>

## Algorithm: Pirates of the RAG

```pseudo
# Inputs:
#   f*             â€” LLM
#   e*             â€” text encoder
#   sim            â€” similarity fn
#   Î±â‚, Î±â‚‚         â€” similarity thresholds
#   C              â€” injection commands
#   a              â€” initial anchor
#   Î² > 0          â€” initial relevance
#   n â‰¥ 1          â€” number of anchors to sample

# Initialization
t â† 0
Aâ‚€ â† { a }
Râ‚€ â† { Î² }
K*â‚€ â† âˆ…

# Attack loop
while max(R_t) > 0 do
  t â† t + 1

  ## 1. Sample anchors by relevance
  Ã‚ â† sample(A_t, R_t, n)

  ## 2. Craft and inject queries until we get at least one response chunk
  qâ€² â† generate_base_query(Ã‚, f*)
  S â† âˆ…
  while S = âˆ… do
    q â† inject(qâ€², next(C))
    y â† f(q)
    S â† parse(y)
  end while

  ## 3. Filter out duplicates and accumulate new chunks
  Åœ â† duplicates(S, K*â‚œ, e*, sim, Î±â‚)
  K*â‚œâ‚Šâ‚ â† K*â‚œ âˆª (S \ Åœ)

  ## 4. Extract new anchors and dedupe
  A_new â† extract_anchors(S \ Åœ, f*)
  Ãƒ â† A_new \ duplicates(A_new, Aâ‚œ, e*, sim, Î±â‚‚)
  Aâ‚œâ‚Šâ‚ â† Aâ‚œ âˆª Ãƒ

  ## 5. Update relevance scores
  Î“ â† compute_penalties(Åœ, Aâ‚œ, e*, sim)
  á¹¼ â† extract_anchors(Åœ, f*)
  Râ‚œâ‚Šâ‚ â† update_relevances(Râ‚œ, Ãƒ, á¹¼ \ Ãƒ, Î“)
end while
```

</details>

---

## ğŸ› ï¸ Prerequisites

* **Python** â‰¥ 3.9
* **CUDAâ€‘enabled NVIDIA GPU** (â‰¥ 8â€¯GB VRAM (for the Agent), â‰¥ 4 GB VRAM (for the Attacker))
* **Virtual environment manager** (conda or venv)

---

## ğŸ’¾ Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-username/pirates-of-the-rag.git
   cd pirates-of-the-rag
   ```

2. **Create & activate a virtual environment**

   ```bash
   # Conda
   conda create -n pirate_rag python=3.9  
   conda activate pirate_rag

   # Or venv
   python3 -m venv venv  
   source venv/bin/activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ—„ï¸ Preparing Knowledge Bases

Download and preprocess the datasets (e.g., HealthcareMagic, Miniâ€‘Wikipedia, Miniâ€‘BioASQ) according to the provided scripts. Place raw data under `data/` and run any preprocessing utilities to populate your ChromaDB stores.

---

## ğŸš€ Running Experiments

### 1. Launch Target RAG Agents

Start each target agent in a separate terminal:

```bash
# Agent A: ChatDoctor
python vLLMApi.py --agent_name chatdoctor --port 8010

# Agent B: Wikipedia
python vLLMApi.py --agent_name wikipedia --port 8030

# Agent C: BioASQ
python vLLMApi.py --agent_name bioasq --port 8020
```

### 2. Launch Attacker's Oracle LLM

In a new terminal, run the local oracle model:

```bash
python vLLMApi.py --agent_name oracle --port 8000
```

### 3. Run the Attack

Execute the main script with your desired parameters:

```bash
python main.py \
  --chat_agent wikipedia \
  --nr_topics_to_use 3 \
  --anchor_beta 1 \
  --anchor_similarity_threshold 0.8 \
  --oracolo Llama-3.2-1B
```

Logs and outputs will be saved under `experiments/[experiment_id]/`.

### 4. Analyze Results

Process results and generate metrics/plots:

```bash
python stats.py --experiment_id [YOUR_EXPERIMENT_ID]
```

Plots, statistics, and leak metrics will appear in `experiments/[experiment_id]/`.

---

## ğŸ›¡ï¸ Testing Llama Guard Defense

1. **Launch Guarded Agent**

   ```bash
   # Example for Wikipedia
   python vLLMApi_guard.py --agent_name wikipedia --port 8030
   ```

2. **Run Test Script**

   ```bash
   python test_llama_guard.py
   ```

Safety statistics will be stored in the corresponding experiment directory.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ data/                    # Raw and processed datasets
â”œâ”€â”€ experiments/            # Logs, stats, plots, and saved models
â”‚   â””â”€â”€ [experiment_id]/
â”œâ”€â”€ utils/                  # Prompt templates, helper scripts, etc.
â”œâ”€â”€ DGEA/                   # UnleashingWorms implementation
â”œâ”€â”€ RThief/                 # RagThief implementation
â”œâ”€â”€ Random/                 # PIDE, TGTB, GPTGEN baselines
â”œâ”€â”€ main.py                 # Launch point for Pirate attack
â”œâ”€â”€ vLLMApi.py              # Server for target agents & oracle
â”œâ”€â”€ vLLMApi_guard.py        # Server with Llama Guard enabled
â”œâ”€â”€ test_llama_guard.py     # Evaluate guard efficacy
â”œâ”€â”€ stats.py                # Generate experiment metrics & plots
â”œâ”€â”€ KnowledgeBase.py        # ChromaDB management
â”œâ”€â”€ CustomEmbedder.py       # Embedding utilities
â””â”€â”€ README.md               # This file
```

---

## ğŸ“‘ Citation

If you use this work, please cite our paper:

```bibtex
@article{di2024pirates,
  title={Pirates of the rag: Adaptively attacking llms to leak knowledge bases},
  author={Di Maio, Christian and Cosci, Cristian and Maggini, Marco and Poggioni, Valentina and Melacci, Stefano},
  journal={arXiv preprint arXiv:2412.18295},
  year={2024}
}
```

---

## âš ï¸ Ethical Considerations

This code is released **for academic and research purposes only**. Our goal is to highlight vulnerabilities in RAG systems and foster the development of stronger privacyâ€‘preserving defenses. **Malicious use is strongly discouraged.**

---

## ğŸ“œ License

This project is licensed under the **MIT License**. See [LICENSE](LICENSE) for details.
