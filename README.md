# Pirates of the RAG
**Adaptively Attacking LLMs to Leak Knowledge Bases**

> *Accepted at European Conference on Artificial Intelligence (ECAI) 2025*

[![ECAI 2025](https://img.shields.io/badge/ECAI-2025-blue)](#)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

This repository provides the official implementation of our paper: **"Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases"**. We present a novel, blackâ€‘box, adaptive, and fully automated attack against Retrievalâ€‘Augmented Generation (RAG) systems, demonstrating how to extract private knowledge-base contents via relevanceâ€‘driven query crafting.

<p align="center">
  <img src="https://github.com/gnekt/PiratesOfTheRAG/imgs/6e4eca81.png" alt="Attack Overview" width="100%">
</p>

## ğŸ“– Table of Contents

- [Pirates of the RAG](#pirates-of-the-rag)
  - [ğŸ“– Table of Contents](#-table-of-contents)
  - [ğŸ”‘ Key Features](#-key-features)
  - [ğŸ› ï¸ Prerequisites](#ï¸-prerequisites)
  - [ğŸ’¾ Installation](#-installation)
  - [ğŸ—„ï¸ Preparing Knowledge Bases](#ï¸-preparing-knowledge-bases)
  - [ğŸš€ Running Experiments](#-running-experiments)
    - [1. Launch Target RAG Agents](#1-launch-target-rag-agents)
    - [2. Launch Attacker's Oracle LLM](#2-launch-attackers-oracle-llm)
    - [3. Run the Attack](#3-run-the-attack)
    - [4. Analyze Results](#4-analyze-results)
  - [ğŸ›¡ï¸ Testing Llama Guard Defense](#ï¸-testing-llama-guard-defense)
  - [ğŸ“‚ Repository Structure](#-repository-structure)
  - [ğŸ“‘ Citation](#-citation)
  - [âš ï¸ Ethical Considerations](#ï¸-ethical-considerations)
  - [ğŸ“œ License](#-license)

---

## ğŸ”‘ Key Features

* **Fully Automated & Adaptive**: Runs without human supervision, adapting queries dynamically based on leaked data.
* **Blackâ€‘Box Approach**: Requires no internal knowledge of the target RAGâ€™s architecture or prompts.
* **Openâ€‘Source & Locally Runnable**: Powered by consumerâ€‘grade open models (e.g., Llama 3.2 1B) and embedder, no proprietary APIs.
* **Relevanceâ€‘Based Anchor Mechanism**: Uses dynamically updated topic anchors to guide exploration of hidden KB chunks.
* **Proven Effectiveness**: Outperforms existing extraction methods across multiple RAG configurations.

---

## ğŸ› ï¸ Prerequisites

* **Python** â‰¥ 3.9
* **CUDAâ€‘enabled NVIDIA GPU** (â‰¥ 8â€¯GB VRAM (for the Agent), â‰¥ 4 GB VRAM (for the Attacker))
* **Virtual environment manager** (conda or venv)

---

## ğŸ’¾ Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-username/pirates-of-the-rag.git
   cd pirates-of-the-rag
   ```

2. **Create & activate a virtual environment**

   ```bash
   # Conda
   conda create -n pirate_rag python=3.9  
   conda activate pirate_rag

   # Or venv
   python3 -m venv venv  
   source venv/bin/activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ—„ï¸ Preparing Knowledge Bases

Download and preprocess the datasets (e.g., HealthcareMagic, Miniâ€‘Wikipedia, Miniâ€‘BioASQ) according to the provided scripts. Place raw data under `data/` and run any preprocessing utilities to populate your ChromaDB stores.

---

## ğŸš€ Running Experiments

### 1. Launch Target RAG Agents

Start each target agent in a separate terminal:

```bash
# Agent A: ChatDoctor
python vLLMApi.py --agent_name chatdoctor --port 8010

# Agent B: Wikipedia
python vLLMApi.py --agent_name wikipedia --port 8030

# Agent C: BioASQ
python vLLMApi.py --agent_name bioasq --port 8020
```

### 2. Launch Attacker's Oracle LLM

In a new terminal, run the local oracle model:

```bash
python vLLMApi.py --agent_name oracle --port 8000
```

### 3. Run the Attack

Execute the main script with your desired parameters:

```bash
python main.py \
  --chat_agent wikipedia \
  --nr_topics_to_use 3 \
  --anchor_beta 1 \
  --anchor_similarity_threshold 0.8 \
  --oracolo Llama-3.2-1B
```

Logs and outputs will be saved under `experiments/[experiment_id]/`.

### 4. Analyze Results

Process results and generate metrics/plots:

```bash
python stats.py --experiment_id [YOUR_EXPERIMENT_ID]
```

Plots, statistics, and leak metrics will appear in `experiments/[experiment_id]/`.

---

## ğŸ›¡ï¸ Testing Llama Guard Defense

1. **Launch Guarded Agent**

   ```bash
   # Example for Wikipedia
   python vLLMApi_guard.py --agent_name wikipedia --port 8030
   ```

2. **Run Test Script**

   ```bash
   python test_llama_guard.py
   ```

Safety statistics will be stored in the corresponding experiment directory.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ data/                    # Raw and processed datasets
â”œâ”€â”€ experiments/            # Logs, stats, plots, and saved models
â”‚   â””â”€â”€ [experiment_id]/
â”œâ”€â”€ utils/                  # Prompt templates, helper scripts, etc.
â”œâ”€â”€ DGEA/                   # UnleashingWorms implementation
â”œâ”€â”€ RThief/                 # RagThief implementation
â”œâ”€â”€ Random/                 # PIDE, TGTB, GPTGEN baselines
â”œâ”€â”€ main.py                 # Launch point for Pirate attack
â”œâ”€â”€ vLLMApi.py              # Server for target agents & oracle
â”œâ”€â”€ vLLMApi_guard.py        # Server with Llama Guard enabled
â”œâ”€â”€ test_llama_guard.py     # Evaluate guard efficacy
â”œâ”€â”€ stats.py                # Generate experiment metrics & plots
â”œâ”€â”€ KnowledgeBase.py        # ChromaDB management
â”œâ”€â”€ CustomEmbedder.py       # Embedding utilities
â””â”€â”€ README.md               # This file
```

---

## ğŸ“‘ Citation

If you use this work, please cite our paper:

```bibtex
@inproceedings{pirates-of-the-rag-2025,
  title={Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases},
  author={Di Maio, Christian and Cosci, Cristian and Maggini, Marco and Poggioni, Valentina and Melacci, Stefano},
  booktitle={Proceedings of the European Conference on Artificial Intelligence (ECAI)},
  year={2025},
  url={https://arxiv.org/abs/your-paper-link-here}
}
```

---

## âš ï¸ Ethical Considerations

This code is released **for academic and research purposes only**. Our goal is to highlight vulnerabilities in RAG systems and foster the development of stronger privacyâ€‘preserving defenses. **Malicious use is strongly discouraged.**

---

## ğŸ“œ License

This project is licensed under the **MIT License**. See [LICENSE](LICENSE) for details.
